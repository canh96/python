{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트리의 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정형데이터에 가장 뛰어난 성능을 보이는 모델들 입니다.\n",
    "# 앙상블 모델들은 결정트리(Decision Tree)를 기반으로 만들어졌습니다.\n",
    "# 앙상블 모델들..\n",
    "# - 랜덤포레스트(Rendom Forest)\n",
    "# - 엑스트라 트리(Extra Trees)\n",
    "# - 그레디언트 부스팅(Gradient Boosting)\n",
    "# - 히스토그램 기반 그레디언트 부스팅(Histogram-base Gradient Booting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤포레스트(Random Forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 앙상블 모델 중에 가장 대표격 모델로 사용됨\n",
    "# - 안정적인 성능으로 널리 사용됨\n",
    "# - 앙상블 모델 중에 가장 먼저 시도하는 모델이라고 보면 됩니다.\n",
    "# - 훈련데이터에서 과대적합되는 것을 막아줍니다.\n",
    "# - 검증데이터와 테스트데이터에서 안정적인 성능을 얻을 수 있음\n",
    "\n",
    "### 학습 개념\n",
    "# - 결정트리를 랜덤하게 만들어서 숲을 만든다고 보시면 됩니다.\n",
    "# - 훈련데이터에서 랜덤하게 샘플을 추출하여 훈련을 완료한 후 \n",
    "#   다시 반환을 합니다.\n",
    "# - 랜덤하게 추출 시 이전에 사용된 샘플을 사용할 수도 있음\n",
    "# (중복을 허용함)\n",
    "\n",
    "### 부트스트랩 샘플\n",
    "# - 위에 설명한 랜덤한 샘플 추출 시 중복을 허용하여 데이터를 샘플링 하는 방식\n",
    "# - 샘플 추출 방식\n",
    "# 1. 원본에서 랜덤 샘플 추출\n",
    "# 2. 훈련 이후 사용이 끝나면 원본에 반환\n",
    "# 3. 다시 원본에서 샘플 추출, 이때 중복 값 추출 될 수도 있음\n",
    "#   위 순서를 반복하면서 샘플링을 통해 훈련하는 방식을 랜덤포레스트가 적용하고 있음\n",
    "\n",
    "### *** 랜덤포레스트는 교차검증을 허용 합니다. ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4872, 3) (4872,)\n",
      "(1625, 3) (1625,)\n"
     ]
    }
   ],
   "source": [
    "wine = pd.read_csv('./data/08_wine.csv')\n",
    "\n",
    "\n",
    "data = wine[['alcohol','sugar','pH']].to_numpy()\n",
    "target = wine['class'].to_numpy()\n",
    "\n",
    "train_input,test_input,train_target,test_target = train_test_split(data,target, random_state=42)\n",
    "\n",
    "print(train_input.shape, train_target.shape)\n",
    "print(test_input.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련모델 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤포레스트 클래스(모델) : RandomForestClassifier\n",
    "# 교차검증 : cross_validate()\n",
    "# 교차검증 후 훈련검증결과와 테스트검증결과 확인하기\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#랜덤포레스트 객체생성 : 코어 모두 사용\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "#교차검증 진행\n",
    "# - return_train_score : 검증결과 반환받기\n",
    "scores = cross_validate(rf,train_input, train_target,return_train_score=True,n_jobs=-1)\n",
    "\n",
    "\n",
    "scores\n",
    "#fit_time = 훈련시간\n",
    "#score_time = 검증시간\n",
    "#test_time = 테스트 시간\n",
    "rf.fit(train_input,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997844759088341 0.8914208392565683\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23155241 0.49706658 0.27138101]\n"
     ]
    }
   ],
   "source": [
    "## 특성 중요도 조회하기\n",
    "print(rf.feature_importances_)\n",
    "# 랜덤포레스트는 독립변수를 골고루 쓴다(일반화시키는데 도움을 많이 주는 모델이다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8981937602627258\n"
     ]
    }
   ],
   "source": [
    "###  oob 기능 사용\n",
    "# 훈련에 참여하지 못한 잠여 샘플 사용하는 기능\n",
    "# 기본은 사용안함,\n",
    "rf = RandomForestClassifier(oob_score =True,n_jobs = -1, random_state = 42)\n",
    "rf.fit(train_input,train_target)\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엑스트라 트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 랜덤포레스트와 유사하게 작동\n",
    "# - 기본적으로 100개의 결정트리를 훈련함\n",
    "# - 랜덤포레스트와의 차이점\n",
    "# : 부트스트랩 샘플링을 지원하지 않음\n",
    "# : 훈련데이터 전체를 이용하여 결정트리를 생성\n",
    "# : 무작위로 트리를 분리함\n",
    "# - 사용되는 속성 : splitter = 'random' 무작위 속성\n",
    "# - 장점\n",
    "# : 과대적합을 막고, 검증데이터의 평가 값을 높일 수 있음\n",
    "# :특성 데이터가 많지 않은 경우에는 랜덤포레스트와 큰 차이가 없음\n",
    "# - 랜덤포레스트는 불순도 등 여러가지 조건에 따라 결정트리를 생성하기 때문에\n",
    "# 속도가 느린 반면에 ,\n",
    "# - 엑스트라트리는 랜덤하게 트리를 생성하기에 속도가 다소 빠르다는 장점이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###사용패키지 : 랜덤포레스트와 동일\n",
    "# 사용되는 클래스(모델) : ExtraTreesClassifier\n",
    "\n",
    "### 코어 전체사용, train 및 test 결과값 출력..\n",
    "### 최종 결과인 train 및 test 결과 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997844759088341 0.8903937240035804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "et = ExtraTreesClassifier(n_jobs=-1,random_state=42)\n",
    "scores = cross_validate(et, train_input, train_target,\n",
    "                        return_train_score = True, n_jobs = -1)\n",
    "\n",
    "#최종 훈련평가 결과 및 검증 결과\n",
    "scores\n",
    "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20702369 0.51313261 0.2798437 ]\n"
     ]
    }
   ],
   "source": [
    "et.fit(train_input,train_target)\n",
    "print(et.feature_importances_)\n",
    "# [0.23155241 0.49706658 0.27138101] (랜덤포레스트)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그레디언트 부스팅(Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 깊이(max_depth)가 얕은 결정트리를 사용함\n",
    "# - 기본적으로 amx_depth=3을 사용\n",
    "# - 결정트리는 100개 사용\n",
    "### **** 기존에 다른 훈련모델의 결과가 좋지 않을 때 사용하는 모델 ****\n",
    "# 기존 훈련모델의 오차를 많이 보완해 줍니다.\n",
    "#성능 향상을 위한 모델로 주로 사용됩니다.\n",
    "#과대적합에 강하며, 일반화(과대/과소적합이 없는 상태)에 강합니다.\n",
    "\n",
    "#성능 향상 테스트 방법\n",
    "# - 결정트리의 갯수를 조절하면서 테스트 진행\n",
    "# - 학습률을 지원하기 때문에 학습률의 값을 증가시키면서 테스트 진행\n",
    "#  :기본 학습률은 0.1\n",
    "\n",
    "#단점\n",
    "# - 순서대로 트리를 추가(랜덤하지 않음) 하지 않기 때문에\n",
    "# - 이런 느린 속도를 개선한 모델임\n",
    "#  \"히스토그램 기반 그레디언트 부스팅' 모델임\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그레디언트 부스팅 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8894704231708938 0.8715107671247301\n"
     ]
    }
   ],
   "source": [
    "### 사용하는 클래스(모델) : GradientBoostingClassifier\n",
    "# 객체 생성시 아무것도 안주고 seed값만 줍니다.\n",
    "# 교차 검증시에는 train, test 결과값 출력 합니다.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb=GradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(gb,train_input,train_target,\n",
    "                        return_train_score=True,n_jobs=-1)\n",
    "\n",
    "scores\n",
    "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12517641 0.73300095 0.14182264]\n"
     ]
    }
   ],
   "source": [
    "gb.fit(train_input,train_target)\n",
    "print(gb.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습률 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8894704231708938 0.8715107671247301\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 커지면 트리 보정을 강하게 하기 때문에,\n",
    "# 복잡한 모델을 만들어서 일반화 성능을 떨어뜨리게 된다.\n",
    "# 학습률 : learning rate =0.1 기본값..\n",
    "\n",
    "#learnin_rate는 가급적 안건들이는게 좋다(건들이면 과대적합이 잘 일어남)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gd=GradientBoostingClassifier(n_estimators=100,\n",
    "                              learning_rate = 0.1,\n",
    "                              random_state=42)\n",
    "scores = cross_validate(gd,train_input,train_target,\n",
    "                        return_train_score=True,n_jobs=-1)\n",
    "\n",
    "scores\n",
    "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 히스토그램 기반 그레디언트 부스팅\n",
    "##### - Histogram-base Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9380129799494501 0.8805410414363187\n"
     ]
    }
   ],
   "source": [
    "## 사용하는 클래스(모델) : HistogramBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgb=HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "scores = cross_validate(hgb, train_input,train_target,\n",
    "                        return_train_score =True,n_jobs=-1)\n",
    "\n",
    "scores\n",
    "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8584615384615385"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgb.fit(train_input,train_target)\n",
    "hgb.score(test_input,test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런 이외 다른 패키지에서 지원하는\n",
    "## 히스토그램 기반 그레디언트 부스팅 기능 모델들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9614122399872658 0.8834151529510873\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
    "scores = cross_validate(xgb, train_input,train_target,\n",
    "                        return_train_score =True,n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "##### - 마이크로소프트에서 만든 히스토그램 기반 그레디언트 부스트 패키지\n",
    "##### - 훈련 속도가 매우 빠름\n",
    "##### - 최신 기술을 많이 적용하고 있어서, 인기가 올라가고 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9413484712095832 0.8846461327857632\n"
     ]
    }
   ],
   "source": [
    "lgb=LGBMClassifier()\n",
    "\n",
    "scores = cross_validate(lgb, train_input,train_target,\n",
    "                        return_train_score =True,n_jobs=-1)\n",
    "\n",
    "scores\n",
    "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "         1.065e+03],\n",
       "        [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "         1.050e+03],\n",
       "        [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "         1.185e+03],\n",
       "        ...,\n",
       "        [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "         8.350e+02],\n",
       "        [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "         8.400e+02],\n",
       "        [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "         5.600e+02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='<U7'),\n",
       " 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 178 (50 in each of three classes)\\n    :Number of Attributes: 13 numeric, predictive attributes and the class\\n    :Attribute Information:\\n \\t\\t- Alcohol\\n \\t\\t- Malic acid\\n \\t\\t- Ash\\n\\t\\t- Alcalinity of ash  \\n \\t\\t- Magnesium\\n\\t\\t- Total phenols\\n \\t\\t- Flavanoids\\n \\t\\t- Nonflavanoid phenols\\n \\t\\t- Proanthocyanins\\n\\t\\t- Color intensity\\n \\t\\t- Hue\\n \\t\\t- OD280/OD315 of diluted wines\\n \\t\\t- Proline\\n\\n    - class:\\n            - class_0\\n            - class_1\\n            - class_2\\n\\t\\t\\n    :Summary Statistics:\\n    \\n    ============================= ==== ===== ======= =====\\n                                   Min   Max   Mean     SD\\n    ============================= ==== ===== ======= =====\\n    Alcohol:                      11.0  14.8    13.0   0.8\\n    Malic Acid:                   0.74  5.80    2.34  1.12\\n    Ash:                          1.36  3.23    2.36  0.27\\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\\n    Magnesium:                    70.0 162.0    99.7  14.3\\n    Total Phenols:                0.98  3.88    2.29  0.63\\n    Flavanoids:                   0.34  5.08    2.03  1.00\\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\\n    Proanthocyanins:              0.41  3.58    1.59  0.57\\n    Colour Intensity:              1.3  13.0     5.1   2.3\\n    Hue:                          0.48  1.71    0.96  0.23\\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\n    Proline:                       278  1680     746   315\\n    ============================= ==== ===== ======= =====\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners: \\n\\nForina, M. et al, PARVUS - \\nAn Extendible Package for Data Exploration, Classification and Correlation. \\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science. \\n\\n.. topic:: References\\n\\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \\n  Comparison of Classifiers in High Dimensional Settings, \\n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Technometrics). \\n\\n  The data was used with many others for comparing various \\n  classifiers. The classes are separable, though only RDA \\n  has achieved 100% correct classification. \\n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \\n  (All results using the leave-one-out technique) \\n\\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \\n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \\n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Journal of Chemometrics).\\n',\n",
       " 'feature_names': ['alcohol',\n",
       "  'malic_acid',\n",
       "  'ash',\n",
       "  'alcalinity_of_ash',\n",
       "  'magnesium',\n",
       "  'total_phenols',\n",
       "  'flavanoids',\n",
       "  'nonflavanoid_phenols',\n",
       "  'proanthocyanins',\n",
       "  'color_intensity',\n",
       "  'hue',\n",
       "  'od280/od315_of_diluted_wines',\n",
       "  'proline']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [문제] 와인 데이터 사용\n",
    "# 와인의 화학 조성을 사용하여 와인의 종류 예측 (자유롭게)..\n",
    "\n",
    "# ** 특성 이름을 담고 있는 key 값 = feature_names\n",
    "# ** 특성 데이터를 담고 있는 key 값 = data\n",
    "# ** 범주 와인의 종류를 담고 있는 key 값 = target_names\n",
    "#    -범주는 'class_0'과 'class_1'만 사용 (0과 1로 변경하여 사용)\n",
    "#    -(0 =레드와인, 1 = 화이트와인)\n",
    "\n",
    "#알콜(ALcohol)\n",
    "#말산(Malic acid)\n",
    "#회분(Ash)\n",
    "#회분의 알칼리도(Alcalinity of ash)\n",
    "#마그네슘(Magnesium)\n",
    "#총 폴리페놀(Total phenols)\n",
    "#플라보노이드 폴리페놀(Flavanoids)\n",
    "#비 플라보노이드 폴리페놀(Nonflavanoid phenols)\n",
    "#프로안토시아닌(Proanthocyanins)\n",
    "#색상의 강도(Color intensity)\n",
    "#색상(Hue)\n",
    "#희석 와인의 OD280/OD315 비율( OD280/OD315 of diluted wines)\n",
    "#프롤린(Proline)\n",
    "from sklearn.datasets import load_wine\n",
    "wine_all = load_wine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_input = np.array(wine_all['data'][0:130])\n",
    "wine_target = np.array(wine_all['target'][0:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_target, test_target = train_test_split(wine_input,wine_target,test_size = 0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤포레스트 =                  1.0 0.9800000000000001\n",
      "엑스트라 트리 =                 1.0 0.9800000000000001\n",
      "그라디언트 부스트 =             1.0 0.9589473684210527\n",
      "히스토그램 그라디언트 부스트 =  1.0 0.9394736842105263\n",
      "XGBoost =                       1.0 0.9494736842105264\n",
      "LightGBM =                      1.0 0.9294736842105262\n",
      "-------------------------------------------------------\n",
      "랜덤포레스트 =                  [0.25797416 0.01796373 0.02576245 0.03770346 0.09345008 0.02400221\n",
      " 0.12840886 0.01058505 0.01141028 0.13119712 0.01034972 0.0185842\n",
      " 0.23260868]\n",
      "-------------------------------------------------------\n",
      "엑스트라 트리 =                 [0.1962303  0.03211827 0.02784845 0.03858197 0.06318405 0.06798185\n",
      " 0.09340209 0.02839892 0.02447009 0.13508022 0.02039693 0.0327477\n",
      " 0.23955915]\n",
      "-------------------------------------------------------\n",
      "그라디언트 부스트 =             [ 4.09833781e-03  1.90173530e-02  0.00000000e+00  8.95889886e-05\n",
      "  7.33191038e-03  0.00000000e+00 -4.80816527e-18  0.00000000e+00\n",
      "  0.00000000e+00  8.57878049e-02  1.76136401e-03  6.33803604e-18\n",
      "  8.81913641e-01]\n",
      "-------------------------------------------------------\n",
      "히스토그램 그라디언트 부스트 =  0.9696969696969697\n",
      "-------------------------------------------------------\n",
      "XGBoost =                       [0.08865771 0.01801441 0.         0.04805661 0.         0.07279719\n",
      " 0.         0.06947908 0.         0.08917513 0.         0.\n",
      " 0.6138199 ]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "et = ExtraTreesClassifier(n_jobs=-1,random_state=42)\n",
    "gb=GradientBoostingClassifier(random_state=42)\n",
    "hgb=HistGradientBoostingClassifier(random_state=42)\n",
    "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
    "lgb=LGBMClassifier()\n",
    "\n",
    "\n",
    "\n",
    "scores_rf = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "scores_et = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "scores_gb = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "scores_hgb= cross_validate(hgb,train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "scores_xgb= cross_validate(xgb,train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "scores_lgb= cross_validate(lgb,train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "\n",
    "print('랜덤포레스트 =                 ',np.mean(scores_rf['train_score']),np.mean(scores_rf['test_score']))\n",
    "print('엑스트라 트리 =                ',np.mean(scores_et['train_score']),np.mean(scores_et['test_score']))\n",
    "print('그라디언트 부스트 =            ',np.mean(scores_gb['train_score']),np.mean(scores_gb['test_score']))\n",
    "print('히스토그램 그라디언트 부스트 = ',np.mean(scores_hgb['train_score']),np.mean(scores_hgb['test_score']))\n",
    "print('XGBoost =                      ',np.mean(scores_xgb['train_score']),np.mean(scores_xgb['test_score']))\n",
    "print('LightGBM =                     ',np.mean(scores_lgb['train_score']),np.mean(scores_lgb['test_score']))\n",
    "\n",
    "print('-------------------------------------------------------')\n",
    "\n",
    "rf.fit(train_input,train_target)\n",
    "et.fit(train_input,train_target)\n",
    "gb.fit(train_input,train_target)\n",
    "hgb.fit(train_input,train_target)\n",
    "xgb.fit(train_input,train_target)\n",
    "lgb.fit(train_input,train_target)\n",
    "\n",
    "\n",
    "\n",
    "print('랜덤포레스트 =                 ',rf.feature_importances_)\n",
    "print('-------------------------------------------------------')\n",
    "print('엑스트라 트리 =                ',et.feature_importances_)\n",
    "print('-------------------------------------------------------')\n",
    "print('그라디언트 부스트 =            ',gb.feature_importances_)\n",
    "print('-------------------------------------------------------')\n",
    "print('히스토그램 그라디언트 부스트 = ',hgb.score(test_input,test_target))\n",
    "print('-------------------------------------------------------')\n",
    "print('XGBoost =                      ',xgb.feature_importances_)\n",
    "# print(lgb.feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_kernel",
   "language": "python",
   "name": "django"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38ae1ba9371524da054e8e3fbefd778d16b5a8ac7937a3f395010f627bb73919"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
